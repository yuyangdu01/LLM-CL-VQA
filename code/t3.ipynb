{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15537,
     "status": "ok",
     "timestamp": 1693652774648,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "wANYIav28D4U",
    "outputId": "b5ea3b93-64ca-4d93-fdb5-07d243e43f09"
   },
   "source": [
    "# Environment and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "hard_label_weight=0.25\n",
    "DS_weight=0.5\n",
    "DI_weight=0.5\n",
    "T=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1693652775111,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "pPXnpT1R8Mo1"
   },
   "outputs": [],
   "source": [
    "# OS and system path\n",
    "import os\n",
    "os.chdir(\"C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS and system path\n",
    "import sys\n",
    "sys.path.append('C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/Surgical_VQA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning related \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1693652776206,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "4y4NNvR386GH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kxchen\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Environments\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from lib2to3.pytree import convert\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data  import DataLoader\n",
    "from utils import *\n",
    "from dataloaders.dataloaderClassification import *\n",
    "from models.VisualBertClassification import VisualBertClassification\n",
    "from models.VisualBertResMLPClassification import VisualBertResMLPClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693652795353,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "34DLKrvN9vOO"
   },
   "outputs": [],
   "source": [
    "# functions to be used later\n",
    "def seed_everything(seed=27):\n",
    "    '''\n",
    "    Set random seed for reproducible experiments\n",
    "    Inputs: seed number\n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "\n",
    "\n",
    "    for i, (_, visual_features, q, labels) in enumerate(train_dataloader,0):\n",
    "\n",
    "        # prepare questions\n",
    "        questions = []\n",
    "        for question in q: questions.append(question)\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
    "\n",
    "\n",
    "        # GPU / CPU\n",
    "        visual_features = visual_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs, visual_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
    "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "    # loss and acc\n",
    "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
    "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
    "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    file_names = list()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (file_name, visual_features, q, labels) in enumerate(val_loader,0):\n",
    "            # prepare questions\n",
    "            questions = []\n",
    "            for question in q: questions.append(question)\n",
    "            inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
    "\n",
    "            # GPU / CPU\n",
    "            visual_features = visual_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, visual_features)\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
    "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "            for f in file_name: file_names.append(f)\n",
    "\n",
    "    acc = calc_acc(label_true, label_pred)\n",
    "    c_acc = 0.0\n",
    "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
    "\n",
    "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
    "\n",
    "    if save_output:\n",
    "        '''\n",
    "            Saving predictions\n",
    "        '''\n",
    "        if os.path.exists(args.checkpoint_dir + 'text_files') == False:\n",
    "            os.mkdir(args.checkpoint_dir + 'text_files' )\n",
    "        file1 = open(args.checkpoint_dir + 'text_files/labels.txt', 'w')\n",
    "        file1.write(str(label_true))\n",
    "        file1.close()\n",
    "\n",
    "        file1 = open(args.checkpoint_dir + 'text_files/predictions.txt', 'w')\n",
    "        file1.write(str(label_pred))\n",
    "        file1.close()\n",
    "\n",
    "        if args.dataset_type == 'med_vqa':\n",
    "            if args.dataset_cat == 'cat1':\n",
    "                convert_arr = ['cta - ct angiography', 'no', 'us - ultrasound', 'xr - plain film', 'noncontrast', 'yes', 't2', 'ct w/contrast (iv)', 'mr - flair', 'mammograph', 'ct with iv contrast',\n",
    "                            'gi and iv', 't1', 'mr - t2 weighted', 'mr - t1w w/gadolinium', 'contrast', 'iv', 'an - angiogram', 'mra - mr angiography/venography', 'nm - nuclear medicine', 'mr - dwi diffusion weighted',\n",
    "                            'ct - gi & iv contrast', 'ct noncontrast', 'mr - other pulse seq.', 'ct with gi and iv contrast', 'flair', 'mr - t1w w/gd (fat suppressed)', 'ugi - upper gi', 'mr - adc map (app diff coeff)',\n",
    "                            'bas - barium swallow', 'pet - positron emission', 'mr - pdw proton density', 'mr - t1w - noncontrast', 'be - barium enema', 'us-d - doppler ultrasound', 'mr - stir', 'mr - flair w/gd',\n",
    "                            'ct with gi contrast', 'venogram', 'mr t2* gradient,gre,mpgr,swan,swi', 'mr - fiesta', 'ct - myelogram', 'gi', 'sbft - small bowel', 'pet-ct fusion']\n",
    "            elif args.dataset_cat == 'cat2':\n",
    "                convert_arr = ['axial', 'longitudinal', 'coronal', 'lateral', 'ap', 'sagittal', 'mammo - mlo', 'pa', 'mammo - cc', 'transverse', 'mammo - mag cc', 'frontal', 'oblique', '3d reconstruction', 'decubitus', 'mammo - xcc']\n",
    "            else:\n",
    "                convert_arr = ['lung, mediastinum, pleura', 'skull and contents', 'genitourinary', 'spine and contents', 'musculoskeletal', 'heart and great vessels', 'vascular and lymphatic', 'gastrointestinal', 'face, sinuses, and neck', 'breast']\n",
    "        elif args.dataset_type == 'c80':\n",
    "            convert_arr = ['no', 'calot triangle dissection', 'yes', '1', '2', 'gallbladder dissection',\n",
    "                            'clipping cutting', 'gallbladder retraction', '0', 'cleaning coagulation',\n",
    "                            'gallbladder packaging', 'preparation', '3']\n",
    "        elif args.dataset_type == 'm18':\n",
    "            convert_arr = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
    "                            'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
    "                            'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
    "                            'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"Img\", \"Ground Truth\", \"Prediction\"])\n",
    "        for i in range(len(label_true)):\n",
    "            df = df.append({'Img': file_names[i], 'Ground Truth': convert_arr[label_true[i]], 'Prediction': convert_arr[label_pred[i]]}, ignore_index=True)\n",
    "\n",
    "        df.to_csv(args.checkpoint_dir + args.checkpoint_dir.split('/')[1] + '_' + args.checkpoint_dir.split('/')[2] + '_eval.csv')\n",
    "\n",
    "    return (acc, c_acc, precision, recall, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693652795353,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "Oy6LTj2Chx6y"
   },
   "outputs": [],
   "source": [
    "# functions to be used later\n",
    "def validate_18(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "    file_names = list()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (file_name, visual_features, q, labels, _) in enumerate(val_loader,0):\n",
    "            # prepare questions\n",
    "            questions = []\n",
    "            for question in q: questions.append(question)\n",
    "            inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
    "\n",
    "            # GPU / CPU\n",
    "            visual_features = visual_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, visual_features)\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
    "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "            for f in file_name: file_names.append(f)\n",
    "\n",
    "    acc = calc_acc(label_true, label_pred)\n",
    "    c_acc = 0.0\n",
    "    # c_acc = calc_classwise_acc(label_true, label_pred)\n",
    "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
    "\n",
    "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
    "\n",
    "    if save_output:\n",
    "        '''\n",
    "            Saving predictions\n",
    "        '''\n",
    "        if os.path.exists(args.checkpoint_dir + 'text_files') == False:\n",
    "            os.mkdir(args.checkpoint_dir + 'text_files' )\n",
    "        file1 = open(args.checkpoint_dir + 'text_files/labels.txt', 'w')\n",
    "        file1.write(str(label_true))\n",
    "        file1.close()\n",
    "\n",
    "        file1 = open(args.checkpoint_dir + 'text_files/predictions.txt', 'w')\n",
    "        file1.write(str(label_pred))\n",
    "        file1.close()\n",
    "\n",
    "        if args.dataset_type == 'med_vqa':\n",
    "            if args.dataset_cat == 'cat1':\n",
    "                convert_arr = ['cta - ct angiography', 'no', 'us - ultrasound', 'xr - plain film', 'noncontrast', 'yes', 't2', 'ct w/contrast (iv)', 'mr - flair', 'mammograph', 'ct with iv contrast',\n",
    "                            'gi and iv', 't1', 'mr - t2 weighted', 'mr - t1w w/gadolinium', 'contrast', 'iv', 'an - angiogram', 'mra - mr angiography/venography', 'nm - nuclear medicine', 'mr - dwi diffusion weighted',\n",
    "                            'ct - gi & iv contrast', 'ct noncontrast', 'mr - other pulse seq.', 'ct with gi and iv contrast', 'flair', 'mr - t1w w/gd (fat suppressed)', 'ugi - upper gi', 'mr - adc map (app diff coeff)',\n",
    "                            'bas - barium swallow', 'pet - positron emission', 'mr - pdw proton density', 'mr - t1w - noncontrast', 'be - barium enema', 'us-d - doppler ultrasound', 'mr - stir', 'mr - flair w/gd',\n",
    "                            'ct with gi contrast', 'venogram', 'mr t2* gradient,gre,mpgr,swan,swi', 'mr - fiesta', 'ct - myelogram', 'gi', 'sbft - small bowel', 'pet-ct fusion']\n",
    "            elif args.dataset_cat == 'cat2':\n",
    "                convert_arr = ['axial', 'longitudinal', 'coronal', 'lateral', 'ap', 'sagittal', 'mammo - mlo', 'pa', 'mammo - cc', 'transverse', 'mammo - mag cc', 'frontal', 'oblique', '3d reconstruction', 'decubitus', 'mammo - xcc']\n",
    "            else:\n",
    "                convert_arr = ['lung, mediastinum, pleura', 'skull and contents', 'genitourinary', 'spine and contents', 'musculoskeletal', 'heart and great vessels', 'vascular and lymphatic', 'gastrointestinal', 'face, sinuses, and neck', 'breast']\n",
    "        elif args.dataset_type == 'c80':\n",
    "            convert_arr = ['no', 'calot triangle dissection', 'yes', '1', '2', 'gallbladder dissection',\n",
    "                            'clipping cutting', 'gallbladder retraction', '0', 'cleaning coagulation',\n",
    "                            'gallbladder packaging', 'preparation', '3']\n",
    "        elif args.dataset_type == 'm18':\n",
    "            convert_arr = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
    "                            'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
    "                            'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
    "                            'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"Img\", \"Ground Truth\", \"Prediction\"])\n",
    "        for i in range(len(label_true)):\n",
    "            df = df.append({'Img': file_names[i], 'Ground Truth': convert_arr[label_true[i]], 'Prediction': convert_arr[label_pred[i]]}, ignore_index=True)\n",
    "\n",
    "        df.to_csv(args.checkpoint_dir + args.checkpoint_dir.split('/')[1] + '_' + args.checkpoint_dir.split('/')[2] + '_eval.csv')\n",
    "\n",
    "    return (acc, c_acc, precision, recall, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693652795353,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "0fnLvtCt94V1"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693652795353,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "3YtNNqp4Bmxd",
    "outputId": "62457b25-147a-4051-d8e6-e9af0b983108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--emb_dim'], dest='emb_dim', nargs=None, const=None, default=300, type=<class 'int'>, choices=None, required=False, help='dimension of word embeddings.', metavar=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1693652795655,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "wUKH05QrBm0V",
    "outputId": "5a684274-6449-46f3-adc4-08a4fc8078ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--encoder_layers'], dest='encoder_layers', nargs=None, const=None, default=6, type=<class 'int'>, choices=None, required=False, help='the number of layers of encoder in Transformer.', metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
    "parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
    "parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1693652795655,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "rUkuYH5dBm2j",
    "outputId": "22208c70-2fb4-4850-b656-0de0ddf58b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--validate'], dest='validate', nargs=None, const=None, default=False, type=None, choices=None, required=False, help='When only validation required False/True', metavar=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Training parameters\n",
    "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
    "    parser.add_argument('--batch_size',     type=int,   default=64,                                 help='batch_size')\n",
    "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
    "    parser.add_argument('--print_freq',     type=int,   default=100,                                help='print training/validation stats every __ batches.')\n",
    "\n",
    "    # existing checkpoint\n",
    "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
    "\n",
    "    parser.add_argument('--lr',             type=float, default=0.00001,                            help='0.000005, 0.00001, 0.000005')\n",
    "    parser.add_argument('--checkpoint_dir', default= '/content/drive/MyDrive/Colab Notebooks/research/multi-modality/svqa/checkpoints/18/',    help='med_vqa_c$version$/m18/c80//m18_vid$temporal_size$/c80_vid$temporal_size$') #clf_v1_2_1x1/med_vqa_c3\n",
    "    parser.add_argument('--dataset_type',   default= 'm18',                                     help='med_vqa/m18/c80/m18_vid/c80_vid')\n",
    "    parser.add_argument('--dataset_cat',    default= 'None',                                        help='cat1/cat2/cat3')\n",
    "    parser.add_argument('--transformer_ver',default= 'vbrm',                                        help='vb/vbrm')\n",
    "    parser.add_argument('--tokenizer_ver',  default= 'v2',                                          help='v2/v3')\n",
    "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
    "    parser.add_argument('--temporal_size',  default= 3,                                             help='1/2/3/4/5')\n",
    "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
    "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
    "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1693652849809,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "927spLAMCdd-",
    "outputId": "9150aa37-0cb2-471a-ebaf-ee122b0fd19c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-f'], dest='f', nargs=None, const=None, default=None, type=None, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('-f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693652851302,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "BVYPeC_bBm4_"
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693652851630,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "qT2E7aBP94YS"
   },
   "outputs": [],
   "source": [
    "# load checkpoint, these parameters can't be modified\n",
    "final_args = {\"emb_dim\": 300, \"n_heads\": 8, \"dropout\": 0.1, \"encoder_layers\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693652852602,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "k3p_qRlO9vRA"
   },
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693652853556,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "VUVcmH4N-md_",
    "outputId": "bfd68e40-c842-461c-a140-19e32107696a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "print('device =', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693652856405,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "AtPTsnSQ-mgo"
   },
   "outputs": [],
   "source": [
    "# best model initialize\n",
    "start_epoch = 1\n",
    "best_epoch = [0]\n",
    "best_results = [0.0]\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1340,
     "status": "ok",
     "timestamp": 1693652859334,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "bmnCO1QF-vyO"
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = None\n",
    "tokenizer = BertTokenizer.from_pretrained(\"C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693652859334,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "2Dd9-TLsuoji",
    "outputId": "4367b4c5-75bb-4549-fb30-2dcb7f5a077b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18', vocab_size=237, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_class = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisualBertClassification(vocab_size=len(tokenizer), layers=6, n_heads=8, num_class = args.num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQnFnWfWZ4H8"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1693655027586,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "ZnXYJoVI0TD4"
   },
   "outputs": [],
   "source": [
    "def str2list(target_str):\n",
    "  res=target_str.strip('[')\n",
    "  res=res.strip(']')\n",
    "  res=res.split(',')\n",
    "\n",
    "  for i in range(len(res)):\n",
    "    res[i] = res[i].strip() # remove the space\n",
    "\n",
    "  new_list = [float(x) for x in res]\n",
    "  return new_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1693655043206,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "hKinQRdVaJ5S"
   },
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_lwf(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    label_true = None\n",
    "    label_pred = None\n",
    "    label_score = None\n",
    "\n",
    "\n",
    "    for i, (_, visual_features, q, labels, t5_loss) in enumerate(train_dataloader,0):\n",
    "\n",
    "        label_number = labels.numpy()[0]\n",
    "\n",
    "        # prepare questions\n",
    "        questions = []\n",
    "        for question in q: questions.append(question)\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
    "\n",
    "        # t5 loss\n",
    "        t5_loss_list = []\n",
    "        for j in range(len(t5_loss)):\n",
    "          tmp = str2list(t5_loss[j])\n",
    "          t5_loss_list.append(tmp)\n",
    "        check = np.reciprocal(t5_loss_list)\n",
    "        t5_loss_tensor = torch.tensor(check)\n",
    "\n",
    "        t5_loss_tensor = t5_loss_tensor.to(device)\n",
    "\n",
    "        # GPU / CPU\n",
    "        visual_features = visual_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs, visual_features)\n",
    "        soft_target = model_old(inputs, visual_features)\n",
    "\n",
    "        loss1 = criterion(outputs, labels)\n",
    "\n",
    "        outputs_S = F.softmax(outputs[:,:out_features]/T,dim=1)\n",
    "        outputs_T = F.softmax(soft_target[:,:out_features]/T,dim=1)\n",
    "\n",
    "        outputs_t5_loss = F.softmax(t5_loss_tensor[:,:out_features]/T,dim=1)\n",
    "\n",
    "        loss2 = outputs_T.mul(-1*torch.log(outputs_S))\n",
    "        loss2 = loss2.sum(1)\n",
    "        loss2 = loss2.mean()*T*T\n",
    "\n",
    "        loss3 = outputs_t5_loss.mul(-1*torch.log(outputs_S))\n",
    "        loss3 = loss3.sum(1)\n",
    "        loss3 = loss3.mean()*T*T\n",
    "\n",
    "        loss = loss1 * acc_weight.at[label_number,'weight_true_label'] + loss2 * acc_weight.at[label_number,'weight_soft'] + loss3 * acc_weight.at[label_number,'weight_llm']\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
    "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
    "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
    "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
    "\n",
    "    # loss and acc\n",
    "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
    "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
    "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzqKlmoaNfZU"
   },
   "source": [
    "# Calculate imbalance ratio (IR) for the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question2tool_18(question):\n",
    "    question = question.strip('?') # remove the question mark\n",
    "    split = question.split()\n",
    "    tool = 'error'\n",
    "    for i in range(len(split)):\n",
    "        if split[i] in instrument_18:\n",
    "            tool = split[i]\n",
    "\n",
    "    return tool\n",
    "\n",
    "instrument_18 = ['bipolar_forceps','prograsp_forceps','monopolar_curved_scissors','ultrasound_probe','large_needle_driver','suction','clip_applier','stapler']\n",
    "\n",
    "def question2tool_17(question):\n",
    "    question = question.strip('?') # remove the question mark\n",
    "    split = question.split()\n",
    "    tool = 'error'\n",
    "    for i in range(len(split)):\n",
    "        if split[i] in instrument_17:\n",
    "            tool = split[i]\n",
    "    \n",
    "    return tool\n",
    "\n",
    "instrument_17 = ['bipolar_forceps','prograsp_forceps','monopolar_curved_scissors','ultrasound_probe','large_needle_driver']\n",
    "\n",
    "def question2tool_DAISI(question):\n",
    "    question = question.strip('?') # remove the question mark\n",
    "    split = question.split()\n",
    "    tool = split[-1]\n",
    "    return tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = np.arange(1,380).tolist()\n",
    "val_seq = np.arange(380,475).tolist()\n",
    "\n",
    "folder_head = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/new_data_daisi/seq_'\n",
    "folder_tail = '/vqa/*.txt'\n",
    "\n",
    "train_seq_new = np.arange(1,1500).tolist()\n",
    "val_seq_new = np.arange(1500,1875).tolist()\n",
    "\n",
    "folder_head_new = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/DAISI_v1_0830/seq_'\n",
    "folder_tail_new = '/vqa/*.txt'\n",
    "\n",
    "train_dataset = DAISI_VQA_Combine(train_seq, folder_head, folder_tail, train_seq_new, folder_head_new, folder_tail_new, patch_size = 5)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= 1, shuffle=True)\n",
    "val_dataset = DAISI_VQA_Combine(val_seq, folder_head, folder_tail, val_seq_new, folder_head_new, folder_tail_new, patch_size = 5)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size= 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "a = []\n",
    "\n",
    "for i in range(len(train_dataset.vqas)):\n",
    "    q.append(train_dataset.vqas[i][1].split('|')[0])\n",
    "    a.append(train_dataset.vqas[i][1].split('|')[1])\n",
    "    \n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "c={\"question\" : q,\n",
    "   \"answer\" : a}\n",
    "data_daisi=DataFrame(c)\n",
    "\n",
    "for i in range(len(data_daisi)):\n",
    "    data_daisi.at[i,'q_type'] = question2tool_DAISI(data_daisi.at[i,'question'])\n",
    "\n",
    "data_daisi['type'] = 'daisi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
    "val_seq = [1, 5, 16]\n",
    "\n",
    "folder_head = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/seq_'\n",
    "folder_tail = '/vqa/Classification_t5_loss/*.txt'\n",
    "\n",
    "train_dataset = EndoVis18VQAClassification(train_seq, folder_head, folder_tail, patch_size = 5)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= 1, shuffle=True)\n",
    "\n",
    "val_dataset = EndoVis18VQAClassification(val_seq, folder_head, folder_tail, patch_size = 5)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size= 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "a = []\n",
    "\n",
    "for i in range(len(train_dataset.vqas)):\n",
    "    q.append(train_dataset.vqas[i][1].split('|')[0])\n",
    "    a.append(train_dataset.vqas[i][1].split('|')[1])\n",
    "    \n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "c={\"question\" : q,\n",
    "   \"answer\" : a}\n",
    "data=DataFrame(c)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data.at[i,'q_type'] = question2tool_18(data.at[i,'question'])\n",
    "\n",
    "data['type'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [1,2,6,9]\n",
    "val_seq = [8]\n",
    "\n",
    "folder_head = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis17/seq_'\n",
    "folder_tail = '/vqa/*.txt'\n",
    "\n",
    "train_dataset = EndoVis17VQAClassification(train_seq, folder_head, folder_tail, patch_size = 5)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= 64, shuffle=True)\n",
    "val_dataset = EndoVis17VQAClassification(val_seq, folder_head, folder_tail, patch_size = 5)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size= 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "a = []\n",
    "\n",
    "for i in range(len(train_dataset.vqas)):\n",
    "    q.append(train_dataset.vqas[i][1].split('|')[0])\n",
    "    a.append(train_dataset.vqas[i][1].split('|')[1])\n",
    "    \n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "c={\"question\" : q,\n",
    "   \"answer\" : a}\n",
    "data_17=DataFrame(c)\n",
    "\n",
    "data_17['q_type'] = ''\n",
    "for i in range(len(data_17)):\n",
    "    data_17.at[i,'q_type'] = question2tool_17(data_17.at[i,'question'])\n",
    "\n",
    "data_17['type'] = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = data.append(data_17)\n",
    "data_tmp = data_tmp.append(data_daisi)\n",
    "data_all = data_tmp[data_tmp['q_type']!='error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data_all['q_type'].value_counts().values\n",
    "x=data_all['q_type'].value_counts().index\n",
    "frequency = pd.DataFrame()\n",
    "frequency.index = x\n",
    "frequency['17+18+daisi'] = y\n",
    "\n",
    "y_17=data_all[data_all['type']==17]['q_type'].value_counts().values\n",
    "x=data_all[data_all['type']==17]['q_type'].value_counts().index\n",
    "frequency_17 = pd.DataFrame()\n",
    "frequency_17.index = x\n",
    "frequency_17['17'] = y_17\n",
    "\n",
    "y_daisi=data_all[data_all['type']=='daisi']['q_type'].value_counts().values\n",
    "x=data_all[data_all['type']=='daisi']['q_type'].value_counts().index\n",
    "frequency_daisi = pd.DataFrame()\n",
    "frequency_daisi.index = x\n",
    "frequency_daisi['daisi'] = y_daisi\n",
    "\n",
    "y_18=data_all[data_all['type']==18]['q_type'].value_counts().values\n",
    "x=data_all[data_all['type']==18]['q_type'].value_counts().index\n",
    "frequency_18 = pd.DataFrame()\n",
    "frequency_18.index = x\n",
    "frequency_18['18'] = y_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_all = frequency.join(frequency_18).join(frequency_17).join(frequency_daisi)\n",
    "\n",
    "frequency_all = frequency_all.fillna(0)\n",
    "frequency_all['17+18'] = frequency_all['17'] + frequency_all['18']\n",
    "\n",
    "frequency_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = frequency_all[frequency_all['17+18+daisi']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_17_18_daisi=frequency_all['17+18+daisi'].max()\n",
    "tmp = frequency_all[['17+18+daisi']].values\n",
    "tmp1 = tmp.nonzero()\n",
    "min_17_18_daisi = tmp[tmp1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "IR_17_18_daisi = max_17_18_daisi / min_17_18_daisi\n",
    "ln_IR_17_18_daisi = math.log(IR_17_18_daisi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu8egdk7hllm"
   },
   "source": [
    "# train DAISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2848,
     "status": "ok",
     "timestamp": 1693660701396,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "WM_QCkYOkNAb",
    "outputId": "0caf1a98-6769-48d9-bb86-c3dac5f8b179"
   },
   "outputs": [],
   "source": [
    "train_seq = np.arange(1,380).tolist()\n",
    "val_seq = np.arange(380,475).tolist()\n",
    "\n",
    "folder_head = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/new_data_daisi/seq_'\n",
    "folder_tail = '/vqa/*.txt'\n",
    "\n",
    "train_seq_new = np.arange(1,1500).tolist()\n",
    "val_seq_new = np.arange(1500,1875).tolist()\n",
    "\n",
    "folder_head_new = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/DAISI_v1_0830/seq_'\n",
    "folder_tail_new = '/vqa/*.txt'\n",
    "\n",
    "train_dataset = DAISI_VQA_Combine(train_seq, folder_head, folder_tail, train_seq_new, folder_head_new, folder_tail_new, patch_size = 5)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= 1, shuffle=True)\n",
    "val_dataset = DAISI_VQA_Combine(val_seq, folder_head, folder_tail, val_seq_new, folder_head_new, folder_tail_new, patch_size = 5)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size= 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3962,
     "status": "ok",
     "timestamp": 1693660729981,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "Yt_Kab9-hnU8"
   },
   "outputs": [],
   "source": [
    "# old model\n",
    "checkpoint_old = torch.load('C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/ablation/checkpoints_all/17+18/checkpoint_1718.pth.tar')\n",
    "model_old = checkpoint_old['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1693660731144,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "aa5W7Ki6jJhf"
   },
   "outputs": [],
   "source": [
    "# new model\n",
    "checkpoint = torch.load('C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/ablation/checkpoints_all/17+18/checkpoint_1718.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693660731145,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "9oVbOOqvjJkO"
   },
   "outputs": [],
   "source": [
    "model = checkpoint['model']\n",
    "optimizer = checkpoint['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693660731485,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "Q1q3DMEHr-Cg"
   },
   "outputs": [],
   "source": [
    "# change the last FC layer for new model (add the node for new classes)\n",
    "num_new_class = 2\n",
    "\n",
    "def kaiming_normal_init(m):\n",
    "\tif isinstance(m, nn.Conv2d):\n",
    "\t\tnn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\telif isinstance(m, nn.Linear):\n",
    "\t\tnn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
    "\n",
    "# Old number of input/output channel of the last FC layer in old model\n",
    "in_features = model.classifier.in_features\n",
    "out_features = model.classifier.out_features\n",
    "\n",
    "# Old weight/bias of the last FC layer\n",
    "weight = model.classifier.weight.data\n",
    "bias = model.classifier.bias.data\n",
    "\n",
    "# New number of output channel of the last FC layer in new model\n",
    "new_out_features = num_new_class + out_features\n",
    "\n",
    "# Creat a new FC layer and initial it's weight/bias\n",
    "new_fc = nn.Linear(in_features, new_out_features)\n",
    "kaiming_normal_init(new_fc.weight)\n",
    "new_fc.weight.data[:out_features] = weight\n",
    "new_fc.bias.data[:out_features] = bias\n",
    "\n",
    "# Replace the old FC layer\n",
    "model.classifier = new_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1693660734332,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "Acsh0EXQr-FC"
   },
   "outputs": [],
   "source": [
    "model_old.classifier = new_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693660735646,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "xpF3gtYBjJm-",
    "outputId": "f356f218-09bd-4d9c-b6b6-175e3fd2c698"
   },
   "outputs": [],
   "source": [
    "# Move to GPU, if available\n",
    "model = model.to(device)\n",
    "model_old = model_old.to(device)\n",
    "print(final_args)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print('model params: ', pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1693660737875,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "mR9pPL2tjJp-"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693660739127,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "rFldi_-mjJtW"
   },
   "outputs": [],
   "source": [
    "args.checkpoint_dir = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/ablation/checkpoints_all/17+18+daisi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693660740711,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "CmdWD_AUl9g7"
   },
   "outputs": [],
   "source": [
    "# best model initialize\n",
    "start_epoch = 1\n",
    "best_epoch = [0]\n",
    "best_results = [0.0]\n",
    "epochs_since_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693660744161,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "A-xlo_PUmB6e"
   },
   "outputs": [],
   "source": [
    "out_features = model.classifier.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acc of the conventional teacher and the LLM teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft_target\n",
    "\n",
    "label = []\n",
    "label_soft_list = []\n",
    "\n",
    "for i, (_, visual_features, q, labels, t5_loss) in enumerate(train_dataloader,0):\n",
    "\n",
    "    label_number = labels.numpy()[0]\n",
    "\n",
    "    label += labels.tolist()\n",
    "\n",
    "    # prepare questions\n",
    "    questions = []\n",
    "    for question in q: questions.append(question)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
    "    \n",
    "    # GPU / CPU\n",
    "    visual_features = visual_features.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    soft_target = model_old(inputs, visual_features) # soft_target is the output of the old model\n",
    "    output_class_ranks = torch.argsort(soft_target, dim=-1, descending=True)\n",
    "    \n",
    "    label_soft = []\n",
    "    for j in range(len(output_class_ranks)):\n",
    "        label_soft.append(int(output_class_ranks[j][0]))\n",
    "    \n",
    "    label_soft_list += label_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "c={\"label\" : label,\n",
    "   \"label_soft_list\" : label_soft_list}\n",
    "data=DataFrame(c)\n",
    "\n",
    "acc_soft = []\n",
    "\n",
    "for i in range(20):\n",
    "    label_part = []\n",
    "    label_soft_part = []\n",
    "\n",
    "    for j in range(len(data)):\n",
    "        if data.at[j,'label'] == i:\n",
    "            label_part.append(data.at[j,'label'])\n",
    "            label_soft_part.append(data.at[j,'label_soft_list'])\n",
    "\n",
    "    acc_soft.append(accuracy_score(label_part, label_soft_part))\n",
    "acc_soft = [0 if math.isnan(x) else x for x in acc_soft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ACC of the LLM teacher\n",
    "\n",
    "label = []\n",
    "label_llm_list = []\n",
    "\n",
    "for i, (_, visual_features, q, labels, t5_loss) in enumerate(train_dataloader,0):\n",
    "  label += labels.tolist()\n",
    "\n",
    "  t5_loss_list = []\n",
    "  for j in range(len(t5_loss)):\n",
    "    tmp = str2list(t5_loss[j])\n",
    "    t5_loss_list.append(tmp)\n",
    "\n",
    "  check = np.reciprocal(t5_loss_list)\n",
    "  t5_loss_tensor = torch.tensor(check)\n",
    "  output_class_ranks = torch.argsort(t5_loss_tensor, dim=-1, descending=True)\n",
    "\n",
    "  label_llm = []\n",
    "  for j in range(len(output_class_ranks)):\n",
    "    label_llm.append(int(output_class_ranks[j][0]))\n",
    "\n",
    "  label_llm_list += label_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas.core.frame import DataFrame\n",
    "\n",
    "c={\"label\" : label,\n",
    "   \"label_llm_list\" : label_llm_list}\n",
    "data=DataFrame(c)\n",
    "\n",
    "acc_llm = []\n",
    "\n",
    "for i in range(20):\n",
    "  label_part = []\n",
    "  label_llm_part = []\n",
    "\n",
    "  for j in range(len(data)):\n",
    "    if data.at[j,'label'] == i:\n",
    "      label_part.append(data.at[j,'label'])\n",
    "      label_llm_part.append(data.at[j,'label_llm_list'])\n",
    "        \n",
    "  if len(label_part) == 0:\n",
    "    acc_llm.append(0)\n",
    "  else:\n",
    "    acc_llm.append(accuracy_score(label_part, label_llm_part))\n",
    "  \n",
    "acc_llm = [0 if math.isnan(x) else x for x in acc_llm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c={\"acc_soft\" : acc_soft,\n",
    "   \"acc_llm\" : acc_llm}\n",
    "weight_data_17_18_daisi=DataFrame(c)\n",
    "\n",
    "for i in range(len(weight_data_17_18_daisi)):\n",
    "  if weight_data_17_18_daisi.at[i,'acc_soft'] + weight_data_17_18_daisi.at[i,'acc_llm'] == 0:\n",
    "    weight_data_17_18_daisi.at[i,'DS_soft'] = 0.5*(1 - hard_label_weight)\n",
    "    weight_data_17_18_daisi.at[i,'DS_llm'] = 0.5*(1 - hard_label_weight)\n",
    "  else:\n",
    "    weight_data_17_18_daisi.at[i,'DS_soft'] = (1-hard_label_weight) * weight_data_17_18_daisi.at[i,'acc_soft'] / (weight_data_17_18_daisi.at[i,'acc_soft'] + weight_data_17_18_daisi.at[i,'acc_llm'])\n",
    "    weight_data_17_18_daisi.at[i,'DS_llm'] = (1-hard_label_weight) * weight_data_17_18_daisi.at[i,'acc_llm'] / (weight_data_17_18_daisi.at[i,'acc_soft'] + weight_data_17_18_daisi.at[i,'acc_llm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_data_17_18_daisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight processing\n",
    "weight_data_17_18_daisi['DI_soft']=(1-hard_label_weight) * (0.5 / (1 + IR_17_18_daisi))\n",
    "weight_data_17_18_daisi['DI_llm'] = (1-hard_label_weight) * ((0.5 + IR_17_18_daisi) / (1 + IR_17_18_daisi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_data_17_18_daisi['weight_true_label']=hard_label_weight\n",
    "\n",
    "weight_data_17_18_daisi['weight_soft'] = DS_weight * weight_data_17_18_daisi['DS_soft'] + DI_weight * weight_data_17_18_daisi['DI_soft']\n",
    "weight_data_17_18_daisi['weight_llm'] = DS_weight * weight_data_17_18_daisi['DS_llm'] + DI_weight * weight_data_17_18_daisi['DI_llm']\n",
    "\n",
    "weight_data_17_18_daisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_weight = weight_data_17_18_daisi[['weight_true_label','weight_soft','weight_llm']]\n",
    "acc_weight.weight_data_17_18_daisi = ['weight_true_label','weight_soft','weight_llm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PADLc933l9jR",
    "outputId": "03952d90-3130-44ae-855a-1c52a0b0f170"
   },
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, 20):\n",
    "  if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
    "    adjust_learning_rate(optimizer, 0.8)\n",
    "\n",
    "  # train\n",
    "  #train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
    "  train_acc = train_lwf(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
    "\n",
    "  # validation\n",
    "  test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate_18(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
    "\n",
    "  if test_acc >= best_results[0]:\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    best_results[0] = test_acc\n",
    "    best_epoch[0] = epoch\n",
    "    print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
    "    save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
    "\n",
    "  else:\n",
    "    epochs_since_improvement += 1\n",
    "    print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "\n",
    "  if train_acc >= 1.0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSPA1UkEiSog"
   },
   "source": [
    "# testing process after the CL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693660488199,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "Q6lDNvLal9lk",
    "outputId": "e79c6c92-59ba-4c6f-adbc-d94a46a87b80"
   },
   "outputs": [],
   "source": [
    "train_seq = [1,2,6,9]\n",
    "val_seq = [8]\n",
    "\n",
    "folder_head = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis17/seq_'\n",
    "folder_tail = '/vqa/*.txt'\n",
    "\n",
    "train_dataset = EndoVis17VQAClassification(train_seq, folder_head, folder_tail, patch_size = 5)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size= 64, shuffle=True)\n",
    "\n",
    "val_dataset = EndoVis17VQAClassification(val_seq, folder_head, folder_tail, patch_size = 5)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size= 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3072,
     "status": "ok",
     "timestamp": 1693660494049,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "6AQT5ocXpVQZ",
    "outputId": "942395b6-1ba8-4f64-8295-5d7ea4e0b4a3"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/ablation/checkpoints_all/17+18+daisi/Best.pth.tar')\n",
    "\n",
    "model = checkpoint['model']\n",
    "\n",
    "# Move to GPU, if available\n",
    "model = model.to(device)\n",
    "print(final_args)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print('model params: ', pytorch_total_params)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# validation\n",
    "test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=0, tokenizer = tokenizer, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76105,
     "status": "ok",
     "timestamp": 1693631869301,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "nYfSzky6l9o9",
    "outputId": "8c39260f-0cc3-4841-e2da-3bfcf715f7a4"
   },
   "outputs": [],
   "source": [
    "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
    "val_seq = [1, 5, 16]\n",
    "\n",
    "folder_head = 'C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/seq_'\n",
    "folder_tail = '/vqa/Classification_t5_loss/*.txt'\n",
    "\n",
    "val_dataset = EndoVis18VQAClassification(val_seq, folder_head, folder_tail, patch_size = 5)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size= 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122989,
     "status": "ok",
     "timestamp": 1693631992288,
     "user": {
      "displayName": "kexin chen",
      "userId": "10371452033462374396"
     },
     "user_tz": -480
    },
    "id": "E1_93Ptnq2Yp",
    "outputId": "8a8bff58-1d48-4d7f-e780-9aca08777de9"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('C:/Users/kxchen/Desktop/Yuyang/multi-modality-20230721T022352Z-001/multi-modality/endovis18/ablation/checkpoints_all/17+18+daisi/Best.pth.tar')\n",
    "\n",
    "model = checkpoint['model']\n",
    "\n",
    "# Move to GPU, if available\n",
    "model = model.to(device)\n",
    "print(final_args)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print('model params: ', pytorch_total_params)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# validation\n",
    "test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate_18(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=0, tokenizer = tokenizer, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMGb881y4/aPNndqZa/z8xp",
   "collapsed_sections": [
    "gaxIp59xQQwj"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
